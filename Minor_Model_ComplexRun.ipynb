{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 1. IMPORTS & GLOBAL CONFIG       #\n",
    "##########################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import pickle \n",
    "import speech_recognition as sr\n",
    "from transformers import BertTokenizer, BertModel, TFBertModel\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "import tensorflow_text\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your data is located (adjust as needed)\n",
    "BASE_DIR = r\"/Users/shreyanandini/Desktop/Stress Level Detection From Speech/Audio_Speech_Actors_01-24\"\n",
    "total_files = sum(len(files) for _, _, files in os.walk(BASE_DIR))\n",
    "\n",
    "print(f\"Total number of files: {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########################################\n",
    "# #  STEP 2. DATA LOADING & LABELING       #\n",
    "# ##########################################\n",
    "# # We create a DataFrame to store metadata\n",
    "# data_df = pd.DataFrame(columns=['file_path', 'src', 'actor', 'gender', 'intensity', 'statement', 'repeat', 'emotion','transcript'])\n",
    "\n",
    "# # Check if the base directory exists\n",
    "# if not os.path.isdir(BASE_DIR):\n",
    "#     raise ValueError(f\"BASE_DIR does not exist: {BASE_DIR}\")\n",
    "\n",
    "# dir_list = os.listdir(BASE_DIR)\n",
    "# dir_list.sort()\n",
    "# count = 0\n",
    "# recognizer = sr.Recognizer()\n",
    "\n",
    "\n",
    "# for folder in dir_list:\n",
    "#     folder_path = os.path.join(BASE_DIR, folder)\n",
    "#     transcript = \"\"\n",
    "#     # If it's not a directory, skip\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         continue\n",
    "#     for f in os.listdir(folder_path):\n",
    "#         if not f.endswith('.wav'):\n",
    "#             continue      \n",
    "#         # Example filename pattern: \"03-01-01-01-01-01-01.wav\"\n",
    "#         nm = f.split('.')[0].split('-')         \n",
    "#         #try:\n",
    "#         emotion = int(nm[2])   \n",
    "#         src = int(nm[1])\n",
    "#         actor = int(nm[-1])\n",
    "#         emotion = int(nm[2])\n",
    "        \n",
    "#         if int(actor)%2 == 0:\n",
    "#             gender = \"female\"\n",
    "#         else:\n",
    "#             gender = \"male\"\n",
    "        \n",
    "#         if nm[3] == '01':\n",
    "#             intensity = 0\n",
    "#         else:\n",
    "#             intensity = 1\n",
    "        \n",
    "#         if nm[4] == '01':\n",
    "#             statement = 0\n",
    "#         else:\n",
    "#             statement = 1\n",
    "        \n",
    "#         if nm[5] == '01':\n",
    "#             repeat = 0\n",
    "#         else:\n",
    "#             repeat = 1\n",
    "#         file_path = os.path.join(folder_path, f)\n",
    "#         try:\n",
    "#             with sr.AudioFile(file_path) as source:\n",
    "#                 data = recognizer.record(source)\n",
    "#                 transcript = recognizer.recognize_google(data, key=None)\n",
    "#             #print(\"Transcript:\", transcript)\n",
    "#         except Exception as e:\n",
    "#             print(\"Error during transcription:\", e)\n",
    "#             transcript = \"Could not process audio transcription.\"\n",
    "#             # except (IndexError, ValueError) as e:\n",
    "#             #     print(f\"Skipping file (naming mismatch): {f}\")\n",
    "#             #     continue\n",
    "#             #print(file_path)\n",
    "#             #Ensuring the .wav file exists\n",
    "#         if os.path.isfile(file_path):\n",
    "#             data_df.loc[count] = [file_path, src, actor, gender, intensity, statement, repeat, emotion,transcript]\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.to_csv(\"audio_to_transcript.csv\")\n",
    "data_df = pd.read_csv(\"audio_to_transcript.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.transcript.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(data_df)):\n",
    "#     if data_df.emotion[i] == 1:\n",
    "#         lb = \"_neutral\"\n",
    "#     elif data_df.emotion[i] == 2:\n",
    "#         lb = \"_calm\"\n",
    "#     elif data_df.emotion[i] == 3:\n",
    "#         lb = \"_happy\"\n",
    "#     elif data_df.emotion[i] == 4:\n",
    "#         lb = \"_sad\"\n",
    "#     elif data_df.emotion[i] == 5:\n",
    "#         lb = \"_angry\"\n",
    "#     elif data_df.emotion[i] == 6:\n",
    "#         lb = \"_fearful\"\n",
    "#     elif data_df.emotion[i] == 7:\n",
    "#         lb = \"_disgust\"\n",
    "#     elif data_df.emotion[i] == 8:\n",
    "#         lb = \"_surprised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emotion_to_stress(e):\n",
    "    \"\"\"\n",
    "    In your dataset, 4/6 = high stress -> 2, \n",
    "                     5/7 = low stress -> 1,\n",
    "                    else 0 = no stress\n",
    "    \"\"\"\n",
    "    if e in [4,6]:\n",
    "        return 2\n",
    "    elif e in [5,7]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data_df['stress_label'] = data_df['emotion'].apply(map_emotion_to_stress)\n",
    "\n",
    "print(data_df.head())\n",
    "print(\"Total files:\", len(data_df))\n",
    "\n",
    "# Shuffle dataframe\n",
    "data_df = data_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 3. TRAIN/VAL/TEST SPLIT          #\n",
    "##########################################\n",
    "# Splitting data_df into Training and Temporary Sets(temp_df)\n",
    "train_df, temp_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.2,\n",
    "    stratify=data_df['stress_label'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "# Splitting the Temporary Set into Validation and Test Sets\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['stress_label'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000       # Sampling rate\n",
    "MAX_DURATION_SEC = 120.0  # Maximum audio duration\n",
    "N_MFCC = 40               # Number of MFCC features\n",
    "FIXED_FRAMES = 200        # Fixed number of time frames for padding/truncation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def load_audio(file_path, sr=SAMPLE_RATE, max_duration=MAX_DURATION_SEC):\n",
    "    y, sr = librosa.load(file_path, sr=sr, duration=max_duration)\n",
    "    return y, sr\n",
    "\n",
    "def extract_mfcc(file_path, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    y, sr = load_audio(file_path, sr=sr, max_duration=MAX_DURATION_SEC)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # shape: (n_mfcc, time_frames)\n",
    "    if mfcc.shape[1] < FIXED_FRAMES:\n",
    "        pad_width = FIXED_FRAMES - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :FIXED_FRAMES]\n",
    "    return np.expand_dims(mfcc, axis=-1)  # Add channel dimension\n",
    "\n",
    "def extract_bert_embedding(text):\n",
    "    if not text or text == \"Could not process audio transcription.\":\n",
    "        return np.zeros((768,))\n",
    "    tokens = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = bert_model(**tokens)\n",
    "    return tf.squeeze(outputs.last_hidden_state[:, 0, :], axis=0).numpy()\n",
    "\n",
    "def build_features_and_labels(df):\n",
    "    X_audio_list, X_text_list, y_list = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        transcript = row['transcript']\n",
    "        label = row['stress_label']\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        mfcc = extract_mfcc(file_path)\n",
    "        bert_embedding = extract_bert_embedding(transcript)\n",
    "\n",
    "        X_audio_list.append(mfcc)\n",
    "        X_text_list.append(bert_embedding)\n",
    "        y_list.append(label)\n",
    "\n",
    "    X_audio_arr = np.array(X_audio_list, dtype=np.float32)\n",
    "    X_text_arr = np.array(X_text_list, dtype=np.float32)\n",
    "    y_arr = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    return X_audio_arr, X_text_arr, y_arr\n",
    "\n",
    "# Example usage:\n",
    "X_train_audio, X_train_text, y_train = build_features_and_labels(train_df)\n",
    "X_val_audio, X_val_text, y_val = build_features_and_labels(val_df)\n",
    "X_test_audio, X_test_text, y_test = build_features_and_labels(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 5. BUILD THE CNN MODEL           #\n",
    "##########################################\n",
    "# num_classes = 3  # no-stress, low-stress, high-stress\n",
    "# model = models.Sequential([\n",
    "#     layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu',\n",
    "#                   input_shape=(N_MFCC, FIXED_FRAMES, 1)),\n",
    "#     layers.MaxPooling2D(pool_size=(2,2)),\n",
    "   \n",
    "#     layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D(pool_size=(2,2)),\n",
    "   \n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC, FIXED_FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 5. BUILD THE CNN MODEL           #\n",
    "##########################################\n",
    "num_classes = 3\n",
    "audio_input = Input(shape=(N_MFCC, FIXED_FRAMES, 1), name=\"audio_input\")\n",
    "audio_model = layers.Conv2D(16, (3, 3), activation='relu')(audio_input)\n",
    "audio_model = layers.MaxPooling2D((2, 2))(audio_model)\n",
    "audio_model = layers.Conv2D(32, (3, 3), activation='relu')(audio_model)\n",
    "audio_model = layers.MaxPooling2D((2, 2))(audio_model)\n",
    "audio_model = layers.Flatten()(audio_model)\n",
    "\n",
    "preprocess = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1\",  name='roberta_preprocess')\n",
    "\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1\",  name='roberta_encoder',  trainable=True)\n",
    "\n",
    "text_input = Input(shape=(), name=\"text_input\", dtype=tf.string)\n",
    "preprocessed_text = preprocess(text_input)\n",
    "encoded_text = encoder(preprocessed_text)\n",
    "\n",
    "fusion = layers.Concatenate()([audio_model, encoded_text['pooled_output']])\n",
    "concatenated_length = 768 + 12288\n",
    "fusion = layers.Dense(concatenated_length, activation='relu', kernel_initializer = TruncatedNormal(stddev=0.02),name='preclassifier_1')(fusion)\n",
    "fusion = layers.Dense(concatenated_length, activation='relu', kernel_initializer = TruncatedNormal(stddev=0.02),name='preclassifier_2')(fusion)\n",
    "fusion = layers.Dropout(0.1)(fusion)\n",
    "output = layers.Dense(units=num_classes, kernel_initializer = TruncatedNormal(stddev=0.02), activation='softmax')(fusion)\n",
    "\n",
    "model = models.Model(inputs=[audio_input, text_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.000025), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 6. TRAIN THE MODEL               #\n",
    "##########################################\n",
    "EPOCHS = 50     # adjust as needed\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Metric to monitor (e.g., validation loss)\n",
    "    patience=5,              # Number of epochs to wait before stopping\n",
    "    restore_best_weights=True # Restore the best weights\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    [X_train_audio, X_train_text], y_train_one_hot,\n",
    "    validation_data=([X_val_audio, X_val_text], y_val_one_hot),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 7. EVALUATION & METRICS          #\n",
    "##########################################\n",
    "test_loss, test_acc = model.evaluate([X_test_audio, X_test_text], y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict([X_test_audio, X_test_text])\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Stress', 'Low Stress','High Stress']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluate\n",
    "# ---------------------------\n",
    "loss, acc = model.evaluate([X_test_audio, X_test_text], y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten MFCC for SVM and Random Forest\n",
    "def prepare_flat_features(X_audio, X_text):\n",
    "    X_audio_flat = X_audio.reshape(X_audio.shape[0], -1)  # Flatten MFCC\n",
    "    X_combined = np.concatenate((X_audio_flat, X_text), axis=1)  # Combine audio and text features\n",
    "    return X_combined\n",
    "\n",
    "# Prepare train, validation, and test data\n",
    "X_train_combined = prepare_flat_features(X_train_audio, X_train_text)\n",
    "X_val_combined = prepare_flat_features(X_val_audio, X_val_text)\n",
    "X_test_combined = prepare_flat_features(X_test_audio, X_test_text)\n",
    "\n",
    "# Train and evaluate SVM\n",
    "print(\"Training SVM...\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, random_state=RANDOM_SEED)\n",
    "svm_model.fit(X_train_combined, y_train)\n",
    "\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"SVM F1 Score: {f1_score(y_test, y_pred_svm, average='weighted'):.4f}\")\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "rf_model.fit(X_train_combined, y_train)\n",
    "\n",
    "print(\"Evaluating Random Forest...\")\n",
    "y_pred_rf = rf_model.predict(X_test_combined)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Random Forest F1 Score: {f1_score(y_test, y_pred_rf, average='weighted'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning the evaluation and plotting\n",
    "cnn_predictions = np.argmax(model.predict([X_test_audio, X_test_text]), axis=1)\n",
    "\n",
    "# Recalculating metrics for CNN\n",
    "cnn_accuracy = accuracy_score(y_test, cnn_predictions) * 100\n",
    "cnn_f1_score = f1_score(y_test, cnn_predictions, average='weighted') * 100\n",
    "cnn_conf_matrix = confusion_matrix(y_test, cnn_predictions)\n",
    "\n",
    "# SVM and Random Forest metrics are already available\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm) * 100\n",
    "svm_f1_score = f1_score(y_test, y_pred_svm, average='weighted') * 100\n",
    "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf) * 100\n",
    "rf_f1_score = f1_score(y_test, y_pred_rf, average='weighted') * 100\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Data for plotting\n",
    "models = ['CNN', 'SVM', 'Random Forest']\n",
    "accuracy = [cnn_accuracy, svm_accuracy, rf_accuracy]\n",
    "f1_scores = [cnn_f1_score, svm_f1_score, rf_f1_score]\n",
    "conf_matrices = [cnn_conf_matrix, svm_conf_matrix, rf_conf_matrix]\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(models, accuracy, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "plt.title('Model Accuracy Comparison', fontsize=16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: F1-Score comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(models, f1_scores, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('F1-Score (%)', fontsize=14)\n",
    "plt.title('Model F1-Score Comparison', fontsize=16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "titles = ['CNN Confusion Matrix', 'SVM Confusion Matrix', 'Random Forest Confusion Matrix']\n",
    "for i, ax in enumerate(axes):\n",
    "    cax = ax.matshow(conf_matrices[i], cmap='coolwarm', alpha=0.8)\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    ax.set_title(titles[i], pad=20, fontsize=14)\n",
    "    ax.set_xlabel('Predicted', fontsize=12)\n",
    "    ax.set_ylabel('True', fontsize=12)\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_yticks(range(num_classes))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  STEP 8. SAVE THE MODEL                #\n",
    "##########################################\n",
    "# model.save(\"stress_detector_cnn.h5\")\n",
    "# print(\"Model saved as stress_detector_cnn.h5\")\n",
    "# Optionally, you can also do:\n",
    "# joblib.dump(model, \"stress_detector_cnn.pkl\") \n",
    "\n",
    "pickle.dump(model,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
