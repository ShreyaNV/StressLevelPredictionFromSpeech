{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 12117,
     "status": "ok",
     "timestamp": 1735205082999,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "TZHnLjQmPPr5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyanandini/Desktop/Stress Level Detection From Speech/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 1. IMPORTS & GLOBAL CONFIG       #\n",
    "##########################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import parselmouth\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Directory where your data is located (adjust as needed)\n",
    "BASE_DIR = r\"/Users/shreyanandini/Desktop/Stress Level Detection From Speech/Audio_Speech_Actors_01-24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 1421\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = r\"/Users/shreyanandini/Desktop/Stress Level Detection From Speech/Audio_Speech_Actors_01-24\"\n",
    "total_files = sum(len(files) for _, _, files in os.walk(BASE_DIR))\n",
    "\n",
    "print(f\"Total number of files: {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3155,
     "status": "ok",
     "timestamp": 1735205125243,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "G6AItrDLPPsC",
    "outputId": "eee4fa4a-c817-4ca5-eeae-d312b6f526ce"
   },
   "outputs": [],
   "source": [
    "# ##########################################\n",
    "# #  STEP 2. DATA LOADING & LABELING       #\n",
    "# ##########################################\n",
    "# # We create a DataFrame to store metadata\n",
    "# data_df = pd.DataFrame(columns=['file_path', 'src', 'actor', 'gender', 'intensity', 'statement', 'repeat', 'emotion','transcript'])\n",
    "\n",
    "# # Check if the base directory exists\n",
    "# if not os.path.isdir(BASE_DIR):\n",
    "#     raise ValueError(f\"BASE_DIR does not exist: {BASE_DIR}\")\n",
    "\n",
    "# dir_list = os.listdir(BASE_DIR)\n",
    "# dir_list.sort()\n",
    "# count = 0\n",
    "# recognizer = sr.Recognizer()\n",
    "\n",
    "# def extract_pitch(file_path):\n",
    "#     try:\n",
    "#         y, sr = librosa.load(file_path, sr=None)\n",
    "#         pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "#         pitches = pitches[np.nonzero(pitches)]  # Filter non-zero pitch values\n",
    "#         if len(pitches) > 0:\n",
    "#             return np.mean(pitches)\n",
    "#         else:\n",
    "#             return 0\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting pitch for {file_path}: {e}\")\n",
    "#         return None\n",
    "    \n",
    "# base_directory = \"/Users/shreyanandini/Desktop/Stress Level Detection From Speech/Audio_Speech_Actors_01-24\"\n",
    "# output_data = []\n",
    "\n",
    "# # Traverse through the directories\n",
    "# for subdir in sorted(os.listdir(base_directory)):\n",
    "#     subdir_path = os.path.join(base_directory, subdir)\n",
    "#     if os.path.isdir(subdir_path):\n",
    "#         for file in os.listdir(subdir_path):\n",
    "#             if file.endswith(\".wav\"):\n",
    "#                 file_path = os.path.join(subdir_path, file)\n",
    "#                 try:\n",
    "#                     # Extract features\n",
    "#                     pitch = extract_pitch(file_path)\n",
    "                    \n",
    "#                     # Append results to the data list\n",
    "#                     output_data.append({\n",
    "#                         \"file_path\": subdir_path+'/'+file,\n",
    "#                         \"Pitch\": pitch if pitch is not None else 0\n",
    "#                     })\n",
    "#                     #print(f\"Processed {file_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert the data to a DataFrame and save as CSV\n",
    "# output_df = pd.DataFrame(output_data)\n",
    "# output_df.to_csv(\"audio_to_pitch.csv\", index=False)\n",
    "# print(\"Feature extraction complete. Results saved to audio_to_pitch.csv.\")\n",
    "\n",
    "# for folder in dir_list:\n",
    "#     folder_path = os.path.join(BASE_DIR, folder)\n",
    "#     transcript = \"\"\n",
    "#     # If it's not a directory, skip\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         continue\n",
    "#     for f in os.listdir(folder_path):\n",
    "#         if not f.endswith('.wav'):\n",
    "#             continue      \n",
    "#         # Example filename pattern: \"03-01-01-01-01-01-01.wav\"\n",
    "#         nm = f.split('.')[0].split('-')         \n",
    "#         #try:\n",
    "#         emotion = int(nm[2])   \n",
    "#         src = int(nm[1])\n",
    "#         actor = int(nm[-1])\n",
    "#         emotion = int(nm[2])\n",
    "        \n",
    "#         if int(actor)%2 == 0:\n",
    "#             gender = \"female\"\n",
    "#         else:\n",
    "#             gender = \"male\"\n",
    "        \n",
    "#         if nm[3] == '01':\n",
    "#             intensity = 0\n",
    "#         else:\n",
    "#             intensity = 1\n",
    "        \n",
    "#         if nm[4] == '01':\n",
    "#             statement = 0\n",
    "#         else:\n",
    "#             statement = 1\n",
    "        \n",
    "#         if nm[5] == '01':\n",
    "#             repeat = 0\n",
    "#         else:\n",
    "#             repeat = 1\n",
    "#         file_path = os.path.join(folder_path, f)\n",
    "#         try:\n",
    "#             with sr.AudioFile(file_path) as source:\n",
    "#                 data = recognizer.record(source)\n",
    "#                 transcript = recognizer.recognize_google(data, key=None)\n",
    "#             #print(\"Transcript:\", transcript)\n",
    "#         except Exception as e:\n",
    "#             print(\"Error during transcription:\", e)\n",
    "#             transcript = \"Could not process audio transcription.\"\n",
    "#             # except (IndexError, ValueError) as e:\n",
    "#             #     print(f\"Skipping file (naming mismatch): {f}\")\n",
    "#             #     continue\n",
    "#             #print(file_path)\n",
    "#             #Ensuring the .wav file exists\n",
    "#         if os.path.isfile(file_path):\n",
    "#             data_df.loc[count] = [file_path, src, actor, gender, intensity, statement, repeat, emotion,transcript]\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_path</th>\n",
       "      <th>src</th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repeat</th>\n",
       "      <th>emotion</th>\n",
       "      <th>transcript</th>\n",
       "      <th>stress_label</th>\n",
       "      <th>Pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1094</td>\n",
       "      <td>19</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>can you talking by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1779.4193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>55</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>can you talking by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1818.3252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277</td>\n",
       "      <td>31</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>dogs are sitting by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1828.8231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1368</td>\n",
       "      <td>13</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>talk to city by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1784.7290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>56</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>kids talking by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1804.7471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>1361</td>\n",
       "      <td>1398</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>don't just sitting by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1940.3499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>825</td>\n",
       "      <td>1373</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>kids are talking by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1908.4088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>812</td>\n",
       "      <td>1399</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>kids are talking about the door</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>972</td>\n",
       "      <td>1419</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>talk setting by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1897.3414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>1381</td>\n",
       "      <td>1383</td>\n",
       "      <td>/Users/shreyanandini/Desktop/Stress Level Dete...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>dogs are sitting by the door</td>\n",
       "      <td>0</td>\n",
       "      <td>1774.5052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             1094          19   \n",
       "1              608          55   \n",
       "2              277          31   \n",
       "3             1368          13   \n",
       "4              254          56   \n",
       "...            ...         ...   \n",
       "1415          1361        1398   \n",
       "1416           825        1373   \n",
       "1417           812        1399   \n",
       "1418           972        1419   \n",
       "1419          1381        1383   \n",
       "\n",
       "                                              file_path  src  actor  gender  \\\n",
       "0     /Users/shreyanandini/Desktop/Stress Level Dete...    1      1    male   \n",
       "1     /Users/shreyanandini/Desktop/Stress Level Dete...    1      1    male   \n",
       "2     /Users/shreyanandini/Desktop/Stress Level Dete...    1      1    male   \n",
       "3     /Users/shreyanandini/Desktop/Stress Level Dete...    1      1    male   \n",
       "4     /Users/shreyanandini/Desktop/Stress Level Dete...    1      1    male   \n",
       "...                                                 ...  ...    ...     ...   \n",
       "1415  /Users/shreyanandini/Desktop/Stress Level Dete...    1     24  female   \n",
       "1416  /Users/shreyanandini/Desktop/Stress Level Dete...    1     24  female   \n",
       "1417  /Users/shreyanandini/Desktop/Stress Level Dete...    1     24  female   \n",
       "1418  /Users/shreyanandini/Desktop/Stress Level Dete...    1     24  female   \n",
       "1419  /Users/shreyanandini/Desktop/Stress Level Dete...    1     24  female   \n",
       "\n",
       "      intensity  statement  repeat  emotion                       transcript  \\\n",
       "0             0          0       0        1      can you talking by the door   \n",
       "1             0          0       1        1      can you talking by the door   \n",
       "2             0          1       0        1     dogs are sitting by the door   \n",
       "3             0          1       1        1         talk to city by the door   \n",
       "4             0          0       0        2         kids talking by the door   \n",
       "...         ...        ...     ...      ...                              ...   \n",
       "1415          0          1       1        8   don't just sitting by the door   \n",
       "1416          1          0       0        8     kids are talking by the door   \n",
       "1417          1          0       1        8  kids are talking about the door   \n",
       "1418          1          1       0        8         talk setting by the door   \n",
       "1419          1          1       1        8     dogs are sitting by the door   \n",
       "\n",
       "      stress_label      Pitch  \n",
       "0                0  1779.4193  \n",
       "1                0  1818.3252  \n",
       "2                0  1828.8231  \n",
       "3                0  1784.7290  \n",
       "4                0  1804.7471  \n",
       "...            ...        ...  \n",
       "1415             0  1940.3499  \n",
       "1416             0  1908.4088  \n",
       "1417             0  2001.0757  \n",
       "1418             0  1897.3414  \n",
       "1419             0  1774.5052  \n",
       "\n",
       "[1420 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"audio_to_transcript.csv\")\n",
    "df2 = pd.read_csv('audio_to_pitch.csv')\n",
    "\n",
    "df1_sorted = df1.sort_values(by='file_path').reset_index(drop=True)\n",
    "df2_sorted = df2.sort_values(by='file_path').reset_index(drop=True)\n",
    "\n",
    "# Merge the DataFrames on the 'file_path' column\n",
    "data_df = pd.merge(df1_sorted, df2_sorted, on='file_path', how='inner')\n",
    "\n",
    "# Save the result to a new CSV file if needed\n",
    "data_df.to_csv('audio_voice_features.csv', index=False)\n",
    "\n",
    "# Show the merged DataFrame\n",
    "data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0          1094          19   \n",
      "1           608          55   \n",
      "2           277          31   \n",
      "3          1368          13   \n",
      "4           254          56   \n",
      "\n",
      "                                           file_path  src  actor gender  \\\n",
      "0  /Users/shreyanandini/Desktop/Stress Level Dete...    1      1   male   \n",
      "1  /Users/shreyanandini/Desktop/Stress Level Dete...    1      1   male   \n",
      "2  /Users/shreyanandini/Desktop/Stress Level Dete...    1      1   male   \n",
      "3  /Users/shreyanandini/Desktop/Stress Level Dete...    1      1   male   \n",
      "4  /Users/shreyanandini/Desktop/Stress Level Dete...    1      1   male   \n",
      "\n",
      "   intensity  statement  repeat  emotion                    transcript  \\\n",
      "0          0          0       0        1   can you talking by the door   \n",
      "1          0          0       1        1   can you talking by the door   \n",
      "2          0          1       0        1  dogs are sitting by the door   \n",
      "3          0          1       1        1      talk to city by the door   \n",
      "4          0          0       0        2      kids talking by the door   \n",
      "\n",
      "   stress_label      Pitch  \n",
      "0             0  1779.4193  \n",
      "1             0  1818.3252  \n",
      "2             0  1828.8231  \n",
      "3             0  1784.7290  \n",
      "4             0  1804.7471  \n",
      "Total files: 1420\n"
     ]
    }
   ],
   "source": [
    "def map_emotion_to_stress(e):\n",
    "    \"\"\"\n",
    "    In your dataset, 4/6 = high stress -> 2, \n",
    "                     5/7 = low stress -> 1,\n",
    "                    else 0 = no stress\n",
    "    \"\"\"\n",
    "    if e in [4,6]:\n",
    "        return 2\n",
    "    elif e in [5,7]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data_df['stress_label'] = data_df['emotion'].apply(map_emotion_to_stress)\n",
    "\n",
    "print(data_df.head())\n",
    "print(\"Total files:\", len(data_df))\n",
    "\n",
    "# Shuffle dataframe\n",
    "data_df = data_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1735205130997,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "Ghfr0waYPPsF",
    "outputId": "ca5991ec-f169-4269-8c7d-5cf3a6956939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1136, Val size: 142, Test size: 142\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 3. TRAIN/VAL/TEST SPLIT          #\n",
    "##########################################\n",
    "train_df, temp_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.2,\n",
    "    stratify=data_df['stress_label'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['stress_label'],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190254,
     "status": "ok",
     "timestamp": 1735205325083,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "SDHliBLbPPsG",
    "outputId": "614dc91b-8ce9-4e06-dded-19e06b2de165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for TRAIN set...\n",
      "Extracting features for VAL set...\n",
      "Extracting features for TEST set...\n",
      "X_train shape: (1136, 41, 200, 1)\n",
      "y_train shape: (1136,)\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 4. FEATURE EXTRACTION (MFCC)     #\n",
    "##########################################\n",
    "SAMPLE_RATE = 16000       # or 22050\n",
    "MAX_DURATION_SEC = 5.0    # we’ll pad/truncate to 5s\n",
    "N_MFCC = 40\n",
    "FIXED_FRAMES = 200        # number of frames along time axis\n",
    "\n",
    "def load_audio(file_path, sr=SAMPLE_RATE, max_duration=MAX_DURATION_SEC):\n",
    "    # Load up to max_duration seconds\n",
    "    y, sr = librosa.load(file_path, sr=sr, duration=max_duration)\n",
    "    return y, sr\n",
    "\n",
    "def extract_mfcc(file_path, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    y, sr = load_audio(file_path, sr=sr, max_duration=MAX_DURATION_SEC)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # shape: (n_mfcc, time_frames)\n",
    "\n",
    "    # Pad or truncate to FIXED_FRAMES along the time axis\n",
    "    if mfcc.shape[1] < FIXED_FRAMES:\n",
    "        pad_width = FIXED_FRAMES - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0,0), (0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :FIXED_FRAMES]\n",
    "    return mfcc\n",
    "\n",
    "def build_features_and_labels(df):\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        label = row['stress_label']\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"Warning: file not found -> {file_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Extract MFCC\n",
    "        mfcc = extract_mfcc(file_path)\n",
    "        # Expand dims for CNN: (n_mfcc, time_frames) -> (n_mfcc, time_frames, 1)\n",
    "        mfcc = np.expand_dims(mfcc, axis=-1)\n",
    "\n",
    "        # Process Pitch\n",
    "        pitch = np.array([[row['Pitch']]])  # Shape: (1, 1)\n",
    "        pitch_repeated = np.repeat(pitch, repeats=mfcc.shape[1], axis=1)  # Shape: (1, FIXED_FRAMES)\n",
    "        pitch_repeated = np.expand_dims(pitch_repeated, axis=-1)  # Shape: (1, FIXED_FRAMES, 1)\n",
    "\n",
    "        # Combine MFCC and Pitch along the feature axis\n",
    "        combined_features = np.concatenate((mfcc, pitch_repeated), axis=0)  # Shape: (n_mfcc + 1, FIXED_FRAMES, 1)\n",
    "\n",
    "        X_list.append(combined_features)\n",
    "        y_list.append(label)\n",
    "\n",
    "    X_arr = np.array(X_list, dtype=np.float32)\n",
    "    y_arr = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    return X_arr, y_arr\n",
    "\n",
    "\n",
    "\n",
    "# Build train/val/test feature sets\n",
    "print(\"Extracting features for TRAIN set...\")\n",
    "X_train, y_train = build_features_and_labels(train_df)\n",
    "\n",
    "print(\"Extracting features for VAL set...\")\n",
    "X_val, y_val = build_features_and_labels(val_df)\n",
    "\n",
    "print(\"Extracting features for TEST set...\")\n",
    "X_test, y_test = build_features_and_labels(test_df)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1735205339584,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "bew9y7uNPPsH",
    "outputId": "cc8b53e3-2dba-48d2-d6bd-6c05ee3e1edf"
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1735205345528,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "mE4izFnePPsI",
    "outputId": "b6e5caae-ba09-4b09-975c-eb77841ccc16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyanandini/Desktop/Stress Level Detection From Speech/venv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │   <span style=\"color: #00af00; text-decoration-color: #00af00\">151,007,232</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12288</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,867</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │   \u001b[38;5;34m151,007,232\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12288\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │        \u001b[38;5;34m36,867\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,048,899</span> (576.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m151,048,899\u001b[0m (576.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,048,899</span> (576.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m151,048,899\u001b[0m (576.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 5. BUILD THE CNN MODEL           #\n",
    "##########################################\n",
    "num_classes = 3  \n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(N_MFCC, FIXED_FRAMES, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(12288, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0000001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84738,
     "status": "ok",
     "timestamp": 1735205434902,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "9-5bZFt7PPsJ",
    "outputId": "81448dfa-fde7-4ea1-90c7-5b06b3b61ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 754ms/step - accuracy: 0.4038 - loss: 5.2942 - val_accuracy: 0.4789 - val_loss: 2.5798\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 772ms/step - accuracy: 0.3795 - loss: 5.4598 - val_accuracy: 0.4859 - val_loss: 2.3365\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 733ms/step - accuracy: 0.4017 - loss: 5.3031 - val_accuracy: 0.4930 - val_loss: 2.2351\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 707ms/step - accuracy: 0.4235 - loss: 5.0342 - val_accuracy: 0.5070 - val_loss: 2.2309\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 9s/step - accuracy: 0.4424 - loss: 4.8414 - val_accuracy: 0.4859 - val_loss: 2.2819\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448ms/step - accuracy: 0.4385 - loss: 4.9193 - val_accuracy: 0.5141 - val_loss: 2.2083\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 431ms/step - accuracy: 0.3992 - loss: 5.2107 - val_accuracy: 0.5141 - val_loss: 2.1963\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 422ms/step - accuracy: 0.3880 - loss: 5.2774 - val_accuracy: 0.5141 - val_loss: 2.2292\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 422ms/step - accuracy: 0.4077 - loss: 5.0940 - val_accuracy: 0.5141 - val_loss: 2.2616\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 434ms/step - accuracy: 0.4422 - loss: 4.9233 - val_accuracy: 0.5211 - val_loss: 2.2138\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 480ms/step - accuracy: 0.4455 - loss: 4.7186 - val_accuracy: 0.5352 - val_loss: 2.2443\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 433ms/step - accuracy: 0.4132 - loss: 5.1677 - val_accuracy: 0.5282 - val_loss: 2.1662\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 434ms/step - accuracy: 0.4086 - loss: 5.2179 - val_accuracy: 0.5282 - val_loss: 2.1801\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 430ms/step - accuracy: 0.4164 - loss: 5.0527 - val_accuracy: 0.5282 - val_loss: 2.2988\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 427ms/step - accuracy: 0.4260 - loss: 4.8228 - val_accuracy: 0.5352 - val_loss: 2.1740\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 430ms/step - accuracy: 0.4270 - loss: 4.7499 - val_accuracy: 0.5352 - val_loss: 2.1710\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 426ms/step - accuracy: 0.4392 - loss: 4.6200 - val_accuracy: 0.5352 - val_loss: 2.1923\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 6. TRAIN THE MODEL               #\n",
    "##########################################\n",
    "EPOCHS = 50     # adjust as needed\n",
    "BATCH_SIZE = 64\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Metric to monitor (e.g., validation loss)\n",
    "    patience=5,              # Number of epochs to wait before stopping\n",
    "    restore_best_weights=True # Restore the best weights\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_one_hot,\n",
    "    validation_data=(X_val, y_val_one_hot),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback here\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1735205438358,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "0WxI12t5PPsJ",
    "outputId": "1738c9e6-5137-4860-e365-0995bcb76d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7387 | Test Accuracy: 0.4648\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No Stress       0.51      0.72      0.60        67\n",
      "  Low Stress       0.48      0.43      0.46        37\n",
      " High Stress       0.13      0.05      0.08        38\n",
      "\n",
      "    accuracy                           0.46       142\n",
      "   macro avg       0.38      0.40      0.38       142\n",
      "weighted avg       0.40      0.46      0.42       142\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48 10  9]\n",
      " [17 16  4]\n",
      " [29  7  2]]\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 7. EVALUATION & METRICS          #\n",
    "##########################################\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Stress', 'Low Stress','High Stress']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1735205442629,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "NasEMtjUPPsK",
    "outputId": "0c979071-7470-4c00-cc78-b4223dac4bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7387, Test Accuracy: 46.48%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluate\n",
    "# ---------------------------\n",
    "loss, acc = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1735205447228,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "TAmN33-uPPsL",
    "outputId": "f27fe7c8-6574-4450-e64a-aad57303ee0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as stress_detector_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#  STEP 8. SAVE THE MODEL                #\n",
    "##########################################\n",
    "model.save(\"stress_detector_cnn.h5\")\n",
    "print(\"Model saved as stress_detector_cnn.h5\")\n",
    "\n",
    "# Optionally, you can also do:\n",
    "# joblib.dump(model, \"stress_detector_cnn.pkl\")\n",
    "# if you prefer a pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1735205453410,
     "user": {
      "displayName": "21WH1A0555 GADDAMEEDI USHASHWINI",
      "userId": "02445378269553780684"
     },
     "user_tz": -330
    },
    "id": "8TUnmhgJPPsM"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
